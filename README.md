# Task_05_Descriptive_Stats

For this research task, I worked with a public dataset on cricket players performance, including their batting, bowling, and fielding statistics across multiple seasons. I used Python to run various descriptive analysis queries and compared the results with responses generated by ChatGPT (LLM). The goal was to evaluate whether an LLM could accurately interpret and analyse structured data when asked natural language questions.
I wrote and executed several Python scripts to extract key insights, such as the top run scorers, players with the highest batting averages, most wickets taken, most sixes and fours, and best strike rates. The outputs were precise and matched the expected results based on the dataset. I then prompted ChatGPT with the same questions, and it performed well in most cases especially with simpler queries like "Who are the top 5 run scorers?" and "Who took the most wickets?" where it matched the Python results exactly.
However, there were a few cases where the LLM fell short. For instance, when asked "How many players have at least 1 century?", Python returned a complete list of all players who met the criteria, whereas ChatGPT only listed a few prominent names. This highlights that while LLMs can interpret and summarize well, they may not always return exhaustive results unless specifically prompted to do so.
Lastly, it's worth noting that while this was a relatively small dataset, ChatGPT handled it efficiently. In my past experience working with much larger datasets, Iâ€™ve noticed that LLMs tend to slow down or give incomplete results when dealing with high-volume data. They are helpful for initial exploration and quick summaries but should not replace proper scripting and data validation in large-scale analysis tasks. Overall, this exercise reinforced the usefulness of combining traditional programming tools with LLMs for both accuracy and speed.

